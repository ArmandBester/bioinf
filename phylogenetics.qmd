---
title: "A primer on phylogeny"
---


Before we get to phylogenetics let’s look at some iris data.  As with many other bioinformatic techniques, phylogenetics stems from older methods.  Hierarchical clustering is a form of unsupervised machine learning and unlike some of the other methods of this kind, you do not have to specify the number of clusters you want prior to the analysis.  The picture below illustrates what we will be looking at.

![iris](media/iris-machinelearning.png)

The ‘iris’ dataset is quite famous and is used in many books and tutorials on machine learning.   https://en.wikipedia.org/wiki/Iris_flower_data_set

The data set consists of four different attributes, namely:  sepal length, sepal width, petal length and petal width (all measured in cm).  There are 50 observations for each class and they are like in the image above, versicolor, setosa and virginica.

I could show you the data, but let’s rather look at some fun plots.

```{r, echo=FALSE}
library(GGally)
data("iris")
head(iris, 2)

ggpairs(iris, columns = 1:4, aes(colour = Species))
```


The pairs plotted above plot each attribute or variable against each.  Why would this be useful?


From the above plot, it is now clear that petal length and width are the attributes which are most discriminating between the 3 different flower species.  So if we would like to group these into clusters based on their attributes the above-mentioned would do the best job.  Also, remember that the attributes are measured in cm which means that we can use this numeric data to construct a distance matrix.  Once we have a distance matrix we can create a dendrogram.  So how would we calculate the distance between observations?  Remember we have 150 observations, 50 of each specie.  We would like to calculate the distance of each observation against each.  If you now think about that, if we plot these distances, they should form clusters.

How do we calculate distances?  Remember Pythagoras’s theorem?

![Pythagoras](media/pyth.png)

Or do you remember getting the distance between two points on a cartesian coordinate system, the image below should help.

![cartesian](media/Euclidean_distance-300x240.png)
So how do we calculate the distance (d) between points (x1, y1) and (x2,y2)?

![euclidean](media/euclidean_eq.png)



```{r, echo=FALSE}

suppressPackageStartupMessages(library(dendextend))
suppressPackageStartupMessages(library(circlize))
library(dendextend)
library(colorspace)
library(circlize)

iris_vals <-  iris[,-5]
iris_labels <-  iris[, 5]

iris_col <- rev(rainbow_hcl(3))[as.numeric(iris_labels)]

iris_dist <- dist(iris_vals)
iris_hc <- hclust(iris_dist, method = "complete")
iris_dend <- as.dendrogram(iris_hc)

iris_dend <- rotate(iris_dend, 1:150)
iris_dend <- color_branches(iris_dend, k=3) 

labels_colors(iris_dend) <-
   rainbow_hcl(3)[sort_levels_values(
      as.numeric(iris[,5])[order.dendrogram(iris_dend)]
   )]

labels(iris_dend) <- paste(as.character(iris[,5])[order.dendrogram(iris_dend)],
                           "(",labels(iris_dend),")", 
                           sep = "")

iris_dend <- hang.dendrogram(iris_dend,hang_height=0.1)
iris_dend <- set(iris_dend, "labels_cex", 0.5)

par(mar = c(3,3,3,7))
plot(iris_dend, 
     main = "Hierarchical Clustering Iris data set", 
     horiz =  TRUE,  nodePar = list(cex = .007))

par(mar = rep(0,4))
circlize_dendrogram(iris_dend)
```



















